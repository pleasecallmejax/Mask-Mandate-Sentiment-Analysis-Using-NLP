{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First bring in the necessary import statements to EDA and initial modeling to examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jax/Documents/Flatiron\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import texthero as hr\n",
    "from PIL import Image \n",
    "from wordcloud import WordCloud\n",
    "from nltk import pos_tag, FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize, RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import plot_roc_curve, plot_confusion_matrix, confusion_matrix, classification_report, accuracy_score, precision_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "module_path = os.path.abspath(os.pardir)\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model I'd like to use a count vectorizer and good ole logistic regression. Then I'll use naive bayes which is particularly successful with nlp because it assumes *naively* there is no interdependence amongst the variables. Also Multinomial Bayes allows me to train my model with less data and potentionally mislabeled data. So first let's split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/modeling_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fe8471fe3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/modeling_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/modeling_tweets.csv'"
     ]
    }
   ],
   "source": [
    "unique_df = pd.read_csv('data/modeling_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = unique_df['target'].copy()\n",
    "X = unique_df.drop(columns=['target','existence_confidence'], axis=1).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=1, stratify=y)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X, y, test_size=.25, random_state=2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmed_tweets</th>\n",
       "      <th>stemmed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>lward snow cover washington dc hand obama promise end do year climategate tcot fb</td>\n",
       "      <td>lward snow cover washington dc hand obama promis end did year climateg tcot fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>italy phd programme science management phd programme science management c</td>\n",
       "      <td>itali phd programm scienc manag phd programm scienc manag c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>debate discourage consider human intelligence gw nutcase remain unpersuaded logic evidence</td>\n",
       "      <td>debat discourag consid human intellig gw nutcas remain unpersuad logic evid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>china key fix warmingwith rapid expansion come sizable environmental impact world</td>\n",
       "      <td>china key fix warmingwith rapid expans come sizabl environment impact world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>iceland volcano unlikely slow scientist afp paris • à big volcanic eruption cool</td>\n",
       "      <td>iceland volcano unlik slow scientist afp pari • à big volcan erupt cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>scientific consensus melt way faster glacier tcot</td>\n",
       "      <td>scientif consensu melt way faster glacier tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>look great medium material human impact check witness story wwf</td>\n",
       "      <td>look great media materi human impact check wit stori wwf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>naomi klein cspan cause island extinction year</td>\n",
       "      <td>naomi klein cspan caus island extinct year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>report identifies disease health issue affctd u inclde mental health evrythings stake</td>\n",
       "      <td>report identifi diseas health issu affctd u incld mental health evryth stake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>mlmoore love act like say remind july</td>\n",
       "      <td>mlmoor love act like said remind juli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   lemmed_tweets  \\\n",
       "3601           lward snow cover washington dc hand obama promise end do year climategate tcot fb   \n",
       "411                    italy phd programme science management phd programme science management c   \n",
       "5949  debate discourage consider human intelligence gw nutcase remain unpersuaded logic evidence   \n",
       "1170           china key fix warmingwith rapid expansion come sizable environmental impact world   \n",
       "591             iceland volcano unlikely slow scientist afp paris • à big volcanic eruption cool   \n",
       "...                                                                                          ...   \n",
       "3569                                           scientific consensus melt way faster glacier tcot   \n",
       "2577                             look great medium material human impact check witness story wwf   \n",
       "3547                                              naomi klein cspan cause island extinction year   \n",
       "1215       report identifies disease health issue affctd u inclde mental health evrythings stake   \n",
       "3486                                                       mlmoore love act like say remind july   \n",
       "\n",
       "                                                                      stemmed_tweets  \n",
       "3601  lward snow cover washington dc hand obama promis end did year climateg tcot fb  \n",
       "411                      itali phd programm scienc manag phd programm scienc manag c  \n",
       "5949     debat discourag consid human intellig gw nutcas remain unpersuad logic evid  \n",
       "1170     china key fix warmingwith rapid expans come sizabl environment impact world  \n",
       "591          iceland volcano unlik slow scientist afp pari • à big volcan erupt cool  \n",
       "...                                                                              ...  \n",
       "3569                                  scientif consensu melt way faster glacier tcot  \n",
       "2577                        look great media materi human impact check wit stori wwf  \n",
       "3547                                      naomi klein cspan caus island extinct year  \n",
       "1215    report identifi diseas health issu affctd u incld mental health evryth stake  \n",
       "3486                                           mlmoor love act like said remind juli  \n",
       "\n",
       "[4110 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3601    1\n",
       "411     2\n",
       "5949    0\n",
       "1170    1\n",
       "591     1\n",
       "       ..\n",
       "3569    2\n",
       "2577    1\n",
       "3547    1\n",
       "1215    1\n",
       "3486    0\n",
       "Name: target, Length: 4110, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get to modeling. First let's test out using SMOTE to eliminate our class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_params= {\n",
    "'sm__sampling_strategy' : ['minority', 'not_majority'],\n",
    "'count__decode_error':['ignore', 'replace'],\n",
    "'count__analyzer' : ['word', 'char', 'char_wb'],\n",
    "'model__solver' : ['sag', 'saga'],\n",
    "'model__max_iter' :[4000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_pipe = imbPipeline([('count', CountVectorizer()),\n",
    "                        ('sm', SMOTE()),\n",
    "                        ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_gs = GridSearchCV(imb_pipe, param_grid=imb_params, cv = 5, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.60243309        nan 0.60364964        nan 0.60462287        nan\n",
      " 0.6053528         nan 0.43090024        nan 0.43284672        nan\n",
      " 0.43017032        nan 0.43041363        nan 0.43722628        nan\n",
      " 0.43722628        nan 0.43722628        nan 0.4377129         nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imbmodel = imb_gs.fit(X_train['lemmed_tweets'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89       766\n",
      "           1       0.93      0.94      0.94      2100\n",
      "           2       0.94      0.85      0.89      1244\n",
      "\n",
      "    accuracy                           0.91      4110\n",
      "   macro avg       0.90      0.91      0.91      4110\n",
      "weighted avg       0.92      0.91      0.91      4110\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.62      0.51       256\n",
      "           1       0.73      0.66      0.69       700\n",
      "           2       0.55      0.49      0.52       415\n",
      "\n",
      "    accuracy                           0.60      1371\n",
      "   macro avg       0.57      0.59      0.57      1371\n",
      "weighted avg       0.62      0.60      0.61      1371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imb_best = imbmodel.best_estimator_\n",
    "\n",
    "imby_trn_pred = imb_best.predict(X_train['lemmed_tweets'])\n",
    "imby_tst_pred = imb_best.predict(X_test['lemmed_tweets'])\n",
    "\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, imby_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, imby_tst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, we have not bad results but definitely overfit and not our best. Since we used SMOTE to address the class imbalnce, ideally now we would focus on accuracy since false negatives and positives bear the same weight. Now let's try it without SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params= {\n",
    "'count__decode_error':['strict', 'ignore', 'replace'],\n",
    "'count__analyzer' : ['word', 'char', 'char_wb'],\n",
    "'count__max_df' : [.95],\n",
    "'count__min_df' : [.05],\n",
    "'model__solver' : ['lbfgs','sag', 'saga'],\n",
    "'model__max_iter' :[4000]}\n",
    "    \n",
    "\n",
    "first_pipe = Pipeline([('count', CountVectorizer()),\n",
    "                    ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gs = GridSearchCV(first_pipe, param_grid=cv_params, cv = 5, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "lrmodel = cv_gs.fit(X_train['lemmed_tweets'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.06      0.10       766\n",
      "           1       0.53      0.95      0.68      2100\n",
      "           2       0.50      0.09      0.15      1244\n",
      "\n",
      "    accuracy                           0.52      4110\n",
      "   macro avg       0.49      0.37      0.31      4110\n",
      "weighted avg       0.50      0.52      0.41      4110\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.06      0.11       256\n",
      "           1       0.53      0.94      0.67       700\n",
      "           2       0.43      0.08      0.13       415\n",
      "\n",
      "    accuracy                           0.52      1371\n",
      "   macro avg       0.45      0.36      0.31      1371\n",
      "weighted avg       0.47      0.52      0.41      1371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_best = lrmodel.best_estimator_\n",
    "\n",
    "fy_trn_pred = lr_best.predict(X_train['lemmed_tweets'])\n",
    "fy_tst_pred = lr_best.predict(X_test['lemmed_tweets'])\n",
    "\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, fy_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, fy_tst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holy cajolly wildly overfit to training buuuut a start. Now for my random forest classifier search it took a bit of time as well so be fair warned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param setup for Gridsearch\n",
    "tf_params = {\n",
    " 'tf__max_features':[100, 500, 2000, None],\n",
    " 'tf__ngram_range': [(1, 1), (1, 2), (2, 2), None],\n",
    " 'mnb__alpha': [.1, .5, .8, 1],\n",
    " 'mnb__fit_prior': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([('tf', TfidfVectorizer()),\n",
    "                    ('mnb', MultinomialNB())])\n",
    "mnb_gs = GridSearchCV(mnb_pipe, param_grid=tf_params, cv = 5, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.58783455 0.58588808 0.55596107        nan 0.62068127 0.61849148\n",
      " 0.56618005        nan 0.62992701 0.63333333 0.58199513        nan\n",
      " 0.63284672 0.63430657 0.61216545        nan 0.46423358 0.46472019\n",
      " 0.27664234        nan 0.57518248 0.57518248 0.35523114        nan\n",
      " 0.60559611 0.60389294 0.43260341        nan 0.63309002 0.63576642\n",
      " 0.54476886        nan 0.586618   0.58588808 0.55620438        nan\n",
      " 0.62189781 0.62092457 0.56836983        nan 0.63430657 0.63600973\n",
      " 0.58783455        nan 0.62822384 0.62822384 0.61119221        nan\n",
      " 0.46447689 0.46374696 0.27639903        nan 0.57542579 0.5756691\n",
      " 0.35717762        nan 0.61411192 0.6053528  0.43698297        nan\n",
      " 0.63211679 0.64793187 0.54841849        nan 0.58686131 0.58832117\n",
      " 0.55717762        nan 0.62141119 0.61678832 0.56788321        nan\n",
      " 0.63187348 0.63065693 0.5892944         nan 0.61970803 0.61922141\n",
      " 0.61046229        nan 0.4649635  0.46520681 0.27615572        nan\n",
      " 0.57639903 0.57493917 0.35766423        nan 0.61557178 0.6080292\n",
      " 0.44014599        nan 0.64014599 0.64963504 0.54939173        nan\n",
      " 0.58783455 0.58880779 0.55620438        nan 0.61995134 0.61435523\n",
      " 0.5676399         nan 0.62846715 0.62506083 0.59026764        nan\n",
      " 0.61289538 0.61167883 0.60486618        nan 0.46545012 0.46545012\n",
      " 0.27615572        nan 0.57615572 0.57420925 0.35815085        nan\n",
      " 0.613382   0.60900243 0.44087591        nan 0.64257908 0.64939173\n",
      " 0.54963504        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mnb_model = mnb_gs.fit(X_train['lemmed_tweets'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       766\n",
      "           1       0.94      0.93      0.94      2100\n",
      "           2       0.90      0.89      0.89      1244\n",
      "\n",
      "    accuracy                           0.92      4110\n",
      "   macro avg       0.91      0.92      0.91      4110\n",
      "weighted avg       0.92      0.92      0.92      4110\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.51      0.55       256\n",
      "           1       0.72      0.78      0.75       700\n",
      "           2       0.61      0.58      0.59       415\n",
      "\n",
      "    accuracy                           0.67      1371\n",
      "   macro avg       0.64      0.62      0.63      1371\n",
      "weighted avg       0.66      0.67      0.66      1371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb_best = mnb_model.best_estimator_\n",
    "\n",
    "mnby_trn_pred = mnb_best.predict(X_train['lemmed_tweets'])\n",
    "mnby_tst_pred = mnb_best.predict(X_test['lemmed_tweets'])\n",
    "\n",
    "\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, mnby_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, mnby_tst_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    " 'tf__ngram_range': [(1, 2)],\n",
    " 'tf__max_features':[100, 500, 2000, None],\n",
    " 'tf__ngram_range': [(1, 1), (1, 2), (2, 2), None],\n",
    " 'tf__min_df':[.05, None],\n",
    " 'tf__max_df':[.95, None],\n",
    " 'tf__stop_words': [None, 'english'],\n",
    " 'rf__max_depth': [100, 500, 1000],\n",
    " 'rf__min_samples_split': [100],\n",
    " 'rf__max_leaf_nodes': [None]}\n",
    "\n",
    "rf_pipe = Pipeline([('tf',  TfidfVectorizer()),\n",
    "                     ('rf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.51094891 0.51094891 0.51094891 0.51094891        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.51094891 0.51094891\n",
      " 0.51094891 0.51094891        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.51094891 0.51094891 0.51094891 0.51094891\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.51094891 0.51094891 0.51094891 0.51094891        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.51094891 0.51094891 0.51094891 0.51094891\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.51094891 0.51094891 0.51094891 0.51094891        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.51094891 0.51094891\n",
      " 0.51094891 0.51094891        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.51094891 0.51094891 0.51094891 0.51094891\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.51094891 0.51094891\n",
      " 0.51094891 0.51094891        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.51094891 0.51094891 0.51094891 0.51094891\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.51094891 0.51094891 0.51094891 0.51094891        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.51094891 0.51094891\n",
      " 0.51094891 0.51094891        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "rf_model = rf_gs.fit(X_train['lemmed_tweets'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       766\n",
      "           1       0.51      1.00      0.68      2100\n",
      "           2       0.00      0.00      0.00      1244\n",
      "\n",
      "    accuracy                           0.51      4110\n",
      "   macro avg       0.17      0.33      0.23      4110\n",
      "weighted avg       0.26      0.51      0.35      4110\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       256\n",
      "           1       0.51      1.00      0.68       700\n",
      "           2       0.00      0.00      0.00       415\n",
      "\n",
      "    accuracy                           0.51      1371\n",
      "   macro avg       0.17      0.33      0.23      1371\n",
      "weighted avg       0.26      0.51      0.35      1371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rfy_trn_pred = rf_model.predict(X_train['lemmed_tweets'])\n",
    "rfy_tst_pred = rf_model.predict(X_test['lemmed_tweets'])\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, rfy_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, rfy_tst_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 256,   0],\n",
       "       [  0, 700,   0],\n",
       "       [  0, 415,   0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rfy_tst_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have picked the best model lets get to vectorizing and our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>agency</th>\n",
       "      <th>air</th>\n",
       "      <th>al</th>\n",
       "      <th>allergy</th>\n",
       "      <th>april</th>\n",
       "      <th>bad</th>\n",
       "      <th>believe</th>\n",
       "      <th>blame</th>\n",
       "      <th>blizzard</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>video</th>\n",
       "      <th>volcano</th>\n",
       "      <th>want</th>\n",
       "      <th>washington</th>\n",
       "      <th>way</th>\n",
       "      <th>weather</th>\n",
       "      <th>winter</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4110 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      action  agency  air   al  allergy  april  bad  believe  blame  blizzard  \\\n",
       "0        0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "1        0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "2        0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "3        0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4        0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "...      ...     ...  ...  ...      ...    ...  ...      ...    ...       ...   \n",
       "4105     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4106     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4107     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4108     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4109     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "\n",
       "      ...  use  video   volcano  want  washington       way  weather  winter  \\\n",
       "0     ...  0.0    0.0  0.000000   0.0    0.383548  0.000000      0.0     0.0   \n",
       "1     ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "2     ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "3     ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4     ...  0.0    0.0  0.738926   0.0    0.000000  0.000000      0.0     0.0   \n",
       "...   ...  ...    ...       ...   ...         ...       ...      ...     ...   \n",
       "4105  ...  0.0    0.0  0.000000   0.0    0.000000  0.607029      0.0     0.0   \n",
       "4106  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4107  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4108  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4109  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "\n",
       "         world      year  \n",
       "0     0.000000  0.383548  \n",
       "1     0.000000  0.000000  \n",
       "2     0.000000  0.000000  \n",
       "3     0.436766  0.000000  \n",
       "4     0.000000  0.000000  \n",
       "...        ...       ...  \n",
       "4105  0.000000  0.000000  \n",
       "4106  0.000000  0.000000  \n",
       "4107  0.000000  0.750493  \n",
       "4108  0.000000  0.000000  \n",
       "4109  0.000000  0.000000  \n",
       "\n",
       "[4110 rows x 100 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['lemmed_tweets'])\n",
    "Xtv_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n",
    "Xtv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 5.560221471204316,\n",
       " 'agency': 5.278370319063328,\n",
       " 'air': 5.514759097127559,\n",
       " 'al': 4.964712760208286,\n",
       " 'allergy': 5.537231952979617,\n",
       " 'april': 5.429601288787252,\n",
       " 'bad': 4.706301070056619,\n",
       " 'believe': 5.514759097127559,\n",
       " 'blame': 5.260978576351459,\n",
       " 'blizzard': 4.855513468243295,\n",
       " 'blog': 5.492780190408784,\n",
       " 'bolivia': 5.514759097127559,\n",
       " 'california': 5.351129673345756,\n",
       " 'call': 5.227077024675777,\n",
       " 'carbon': 5.260978576351459,\n",
       " 'cause': 4.602922715602784,\n",
       " 'claim': 5.537231952979617,\n",
       " 'climatechange': 4.9393949522239975,\n",
       " 'climategate': 5.370177868316451,\n",
       " 'clinical': 5.429601288787252,\n",
       " 'cochabamba': 5.560221471204316,\n",
       " 'cold': 5.351129673345756,\n",
       " 'come': 5.278370319063328,\n",
       " 'conference': 4.964712760208286,\n",
       " 'day': 4.778126804627874,\n",
       " 'dc': 4.37266169651971,\n",
       " 'debate': 5.260978576351459,\n",
       " 'do': 5.030962145749487,\n",
       " 'dont': 5.429601288787252,\n",
       " 'earth': 4.736454108227306,\n",
       " 'effect': 5.351129673345756,\n",
       " 'energy': 4.4851396799464,\n",
       " 'environment': 5.514759097127559,\n",
       " 'environmental': 5.632542132783942,\n",
       " 'epa': 5.471273985187819,\n",
       " 'federal': 5.537231952979617,\n",
       " 'fight': 4.767544695297337,\n",
       " 'get': 5.3140884016654075,\n",
       " 'globalwarming': 5.429601288787252,\n",
       " 'go': 5.178286860506346,\n",
       " 'good': 5.194287201852787,\n",
       " 'gop': 5.560221471204316,\n",
       " 'gore': 4.92697243222544,\n",
       " 'government': 5.514759097127559,\n",
       " 'graham': 5.227077024675777,\n",
       " 'great': 5.178286860506346,\n",
       " 'green': 4.4851396799464,\n",
       " 'health': 5.683835427171493,\n",
       " 'help': 5.030962145749487,\n",
       " 'ice': 5.657859940768232,\n",
       " 'immigration': 5.278370319063328,\n",
       " 'impact': 5.194287201852787,\n",
       " 'india': 5.560221471204316,\n",
       " 'issue': 5.3140884016654075,\n",
       " 'just': 4.716251400909787,\n",
       " 'know': 5.429601288787252,\n",
       " 'law': 5.243884142992159,\n",
       " 'legislation': 5.194287201852787,\n",
       " 'like': 5.210547722724567,\n",
       " 'look': 5.514759097127559,\n",
       " 'make': 4.509237231525461,\n",
       " 'mean': 5.3140884016654075,\n",
       " 'melt': 5.58375196861451,\n",
       " 'need': 5.278370319063328,\n",
       " 'new': 3.9277940405455167,\n",
       " 'news': 4.185623149847617,\n",
       " 'obama': 4.726301736763288,\n",
       " 'people': 4.517400542164622,\n",
       " 'policy': 5.537231952979617,\n",
       " 'post': 5.514759097127559,\n",
       " 'real': 5.560221471204316,\n",
       " 'report': 4.493107849595577,\n",
       " 'right': 5.471273985187819,\n",
       " 'say': 4.240017221913416,\n",
       " 'science': 4.469391322978261,\n",
       " 'scientist': 4.550736962432214,\n",
       " 'senate': 5.058741709856563,\n",
       " 'senator': 5.607849520193571,\n",
       " 'snow': 4.150937591859726,\n",
       " 'snowstorm': 5.560221471204316,\n",
       " 'state': 5.3140884016654075,\n",
       " 'stop': 5.351129673345756,\n",
       " 'storm': 5.3895959541735525,\n",
       " 'study': 5.147034317002241,\n",
       " 'tackle': 5.296069896162729,\n",
       " 'talk': 5.072926344848519,\n",
       " 'tcot': 4.271565579648341,\n",
       " 'think': 5.178286860506346,\n",
       " 'time': 4.567831395791513,\n",
       " 'trial': 5.429601288787252,\n",
       " 'use': 5.58375196861451,\n",
       " 'video': 5.194287201852787,\n",
       " 'volcano': 4.990688246611548,\n",
       " 'want': 5.429601288787252,\n",
       " 'washington': 5.227077024675777,\n",
       " 'way': 5.370177868316451,\n",
       " 'weather': 5.116728967506912,\n",
       " 'winter': 5.332437540333604,\n",
       " 'world': 4.517400542164622,\n",
       " 'year': 5.227077024675777}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_values = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>agency</th>\n",
       "      <th>air</th>\n",
       "      <th>al</th>\n",
       "      <th>allergy</th>\n",
       "      <th>april</th>\n",
       "      <th>bad</th>\n",
       "      <th>believe</th>\n",
       "      <th>blame</th>\n",
       "      <th>blizzard</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>video</th>\n",
       "      <th>volcano</th>\n",
       "      <th>want</th>\n",
       "      <th>washington</th>\n",
       "      <th>way</th>\n",
       "      <th>weather</th>\n",
       "      <th>winter</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3503</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4110 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      action  agency  air   al  allergy  april  bad  believe  blame  blizzard  \\\n",
       "1384     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "1402     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "2277     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "5669     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4298     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "...      ...     ...  ...  ...      ...    ...  ...      ...    ...       ...   \n",
       "3503     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4899     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "3131     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "1329     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "4144     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "\n",
       "      ...  use  video   volcano  want  washington       way  weather  winter  \\\n",
       "1384  ...  0.0    0.0  0.000000   0.0    0.383548  0.000000      0.0     0.0   \n",
       "1402  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "2277  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "5669  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4298  ...  0.0    0.0  0.738926   0.0    0.000000  0.000000      0.0     0.0   \n",
       "...   ...  ...    ...       ...   ...         ...       ...      ...     ...   \n",
       "3503  ...  0.0    0.0  0.000000   0.0    0.000000  0.607029      0.0     0.0   \n",
       "4899  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "3131  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "1329  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "4144  ...  0.0    0.0  0.000000   0.0    0.000000  0.000000      0.0     0.0   \n",
       "\n",
       "         world      year  \n",
       "1384  0.000000  0.383548  \n",
       "1402  0.000000  0.000000  \n",
       "2277  0.000000  0.000000  \n",
       "5669  0.436766  0.000000  \n",
       "4298  0.000000  0.000000  \n",
       "...        ...       ...  \n",
       "3503  0.000000  0.000000  \n",
       "4899  0.000000  0.000000  \n",
       "3131  0.000000  0.750493  \n",
       "1329  0.000000  0.000000  \n",
       "4144  0.000000  0.000000  \n",
       "\n",
       "[4110 rows x 100 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_vec = tfidf.fit_transform(X_train['lemmed_tweets'])\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "X_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>agency</th>\n",
       "      <th>air</th>\n",
       "      <th>al</th>\n",
       "      <th>allergy</th>\n",
       "      <th>april</th>\n",
       "      <th>bad</th>\n",
       "      <th>believe</th>\n",
       "      <th>blame</th>\n",
       "      <th>blizzard</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>video</th>\n",
       "      <th>volcano</th>\n",
       "      <th>want</th>\n",
       "      <th>washington</th>\n",
       "      <th>way</th>\n",
       "      <th>weather</th>\n",
       "      <th>winter</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   action  agency  air   al  allergy  april  bad  believe  blame  blizzard  \\\n",
       "0     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "1     0.0     0.0  0.0  0.0      0.0    0.0  0.0      0.0    0.0       0.0   \n",
       "\n",
       "   ...  use  video  volcano  want  washington  way  weather  winter  world  \\\n",
       "0  ...  0.0    0.0      0.0   0.0         0.0  0.0      0.0     0.0    0.0   \n",
       "1  ...  0.0    0.0      0.0   0.0         0.0  0.0      0.0     0.0    0.0   \n",
       "\n",
       "   year  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "#X_val_vec.set_index(y_val.index, inplace=True)\n",
    "X_val_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1371, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d1031034809b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1371, 2]"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_pred=mnb.predict(X_val_vec)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat = mnb.predict(X_val_vec)\n",
    "#print(f1_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train,y_train)\n",
    "#y_pred=model.predict(X_test)\n",
    "#print(accuracy_score(y_test,y_pred))\n",
    "#print(X_train.toarray()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated funcs and values\n",
    "\n",
    "def classify(self, input):\n",
    "    input_text = input_vectorizer(max_features=100)\n",
    "    input_counts = input_vectorizer.transform(input_text)\n",
    "    predictions = self.classifier.predict(input_counts)\n",
    "    print(predictions)\n",
    "    \n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
